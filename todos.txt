OK * Replace PPRJ1 to study.
OK * Rename 'textuals' to orthographic
OK * Translate Turkish comments to English
OK * Check for problematic keywords: islam**, kuran
OK * Replace 'txtsim' texts to osim
OK * Add FastText c++ code FastTestNgramSegmentorWrapper !!!
OK * Remove all TODOs. and #TO:
OK * Setup requirements.txt
OK * Run on linux, github workflow
OK * Make 'Resources' path globally accessible
    OK * path.join instead of Windows \\ paths.
OK * Get rid of 'Paper' path thing.
OK Add cython equivalent code
    * Generator
    * Distance fuctions
    * SegmentedWord
OK * Dependencies
    OK - fix pandas version
    OK - Make sure NLTK version
    OK - Replace goto statement with something else. Load existing ds case.
    OK - Add nltk install
* Provide a readme
    * how to run unit tests
    * how to run for English using NLTK and MorphoLex
    * add cyton and C++ codes are excluded
    * NLTK wordnet installation.
    * Note that dependencies are very limited such as Pandas, NLTK, etc. Most imports are part or Python standard library such as statistics, collections, pathlib, etc.
        * EditDistance is copied form string-sim package.
    * Document dataset run
* Check that I'm exposing the correct functions
    * should I expose the root detection functions for English?
    * Remove unused functions
* NLTKWordNetWrapper elden ge√ßir.
* fix: the bug in WordNet.py file when generating the paper pipeline
* Highlight critical constants
    * Generator: minRootlength, typeDepthRatio etc.

README NOTES
    * Run in src folder to gen requirements.txt:
        pipreqs . --force

AbstractFactory Types
    * WordNet (default NLTK)
    * Language - Grammar
    * RootDetection Stack
    * OrthographicSimilarity alg.
    * Settings/Thresholds
* EditDistance interface ??
* Check final dataset, highlight in readme

IPipelineLanguageProvider:
    factories and parameters.

C:\Users\gokhan\miniconda3\envs\TEST38\python --version